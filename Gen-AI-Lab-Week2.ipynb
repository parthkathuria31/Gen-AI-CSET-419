{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94O8WSeNMihE",
        "outputId": "7f6b7523-8706-45b5-f0c7-6d0e032d25ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/30 | D_loss: 1.413 | D_acc: 52.23% | G_loss: 0.688\n",
            "Epoch 2/30 | D_loss: 1.389 | D_acc: 51.30% | G_loss: 0.736\n",
            "Epoch 3/30 | D_loss: 1.385 | D_acc: 56.16% | G_loss: 0.748\n",
            "Epoch 4/30 | D_loss: 1.383 | D_acc: 60.36% | G_loss: 0.765\n",
            "Epoch 5/30 | D_loss: 1.379 | D_acc: 62.42% | G_loss: 0.775\n",
            "Epoch 6/30 | D_loss: 1.366 | D_acc: 65.51% | G_loss: 0.793\n",
            "Epoch 7/30 | D_loss: 1.362 | D_acc: 66.95% | G_loss: 0.803\n",
            "Epoch 8/30 | D_loss: 1.368 | D_acc: 67.15% | G_loss: 0.814\n",
            "Epoch 9/30 | D_loss: 1.342 | D_acc: 69.60% | G_loss: 0.835\n",
            "Epoch 10/30 | D_loss: 1.347 | D_acc: 68.41% | G_loss: 0.826\n",
            "Epoch 11/30 | D_loss: 1.330 | D_acc: 69.90% | G_loss: 0.875\n",
            "Epoch 12/30 | D_loss: 1.320 | D_acc: 72.01% | G_loss: 0.859\n",
            "Epoch 13/30 | D_loss: 1.286 | D_acc: 73.76% | G_loss: 0.926\n",
            "Epoch 14/30 | D_loss: 1.283 | D_acc: 73.89% | G_loss: 0.933\n",
            "Epoch 15/30 | D_loss: 1.241 | D_acc: 76.87% | G_loss: 1.024\n",
            "Epoch 16/30 | D_loss: 1.235 | D_acc: 77.65% | G_loss: 1.012\n",
            "Epoch 17/30 | D_loss: 1.179 | D_acc: 79.90% | G_loss: 1.127\n",
            "Epoch 18/30 | D_loss: 1.194 | D_acc: 79.81% | G_loss: 1.143\n",
            "Epoch 19/30 | D_loss: 1.107 | D_acc: 81.61% | G_loss: 1.210\n",
            "Epoch 20/30 | D_loss: 0.879 | D_acc: 87.78% | G_loss: 1.661\n",
            "Epoch 21/30 | D_loss: 0.627 | D_acc: 94.55% | G_loss: 2.583\n",
            "Epoch 22/30 | D_loss: 0.539 | D_acc: 96.02% | G_loss: 3.008\n",
            "Epoch 23/30 | D_loss: 0.486 | D_acc: 97.27% | G_loss: 3.118\n",
            "Epoch 24/30 | D_loss: 0.394 | D_acc: 98.91% | G_loss: 4.793\n",
            "Epoch 25/30 | D_loss: 0.396 | D_acc: 99.08% | G_loss: 3.933\n",
            "Epoch 26/30 | D_loss: 0.367 | D_acc: 99.56% | G_loss: 4.902\n",
            "Epoch 27/30 | D_loss: 0.345 | D_acc: 99.80% | G_loss: 6.876\n",
            "Epoch 28/30 | D_loss: 0.330 | D_acc: 100.00% | G_loss: 8.318\n",
            "Epoch 29/30 | D_loss: 0.361 | D_acc: 99.40% | G_loss: 7.785\n",
            "Epoch 30/30 | D_loss: 0.356 | D_acc: 99.53% | G_loss: 5.916\n",
            "\n",
            "Label Distribution of Generated Images:\n",
            "Label 0: 100\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ======================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ======================================================\n",
        "# 2. USER INPUT PARAMETERS\n",
        "# ======================================================\n",
        "dataset_choice = 'mnist'        # 'mnist' or 'fashion'\n",
        "epochs = 30\n",
        "batch_size = 128\n",
        "noise_dim = 100\n",
        "lr_G = 0.0002\n",
        "lr_D = 0.0001\n",
        "save_interval = 5\n",
        "\n",
        "# ======================================================\n",
        "# 3. DATASET LOADING\n",
        "# ======================================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "if dataset_choice == 'mnist':\n",
        "    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "elif dataset_choice == 'fashion':\n",
        "    dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "else:\n",
        "    raise ValueError(\"Invalid dataset choice\")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "img_shape = (1, 28, 28)\n",
        "\n",
        "# ======================================================\n",
        "# 4. OUTPUT FOLDERS\n",
        "# ======================================================\n",
        "os.makedirs(\"generated_samples\", exist_ok=True)\n",
        "os.makedirs(\"final_generated_images\", exist_ok=True)\n",
        "\n",
        "# ======================================================\n",
        "# 5. GENERATOR\n",
        "# ======================================================\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        return img.view(img.size(0), *img_shape)\n",
        "\n",
        "# ======================================================\n",
        "# 6. DISCRIMINATOR\n",
        "# ======================================================\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img = img.view(img.size(0), -1)\n",
        "        return self.model(img)\n",
        "\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "# ======================================================\n",
        "# 7. LOSS & OPTIMIZERS\n",
        "# ======================================================\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
        "\n",
        "# ======================================================\n",
        "# 8. TRAINING LOOP\n",
        "# ======================================================\n",
        "for epoch in range(1, epochs + 1):\n",
        "    D_loss_total, G_loss_total = 0.0, 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch = real_imgs.size(0)\n",
        "\n",
        "        # Label smoothing\n",
        "        real_labels = torch.full((batch, 1), 0.9).to(device)\n",
        "        fake_labels = torch.zeros(batch, 1).to(device)\n",
        "\n",
        "        # --------------------\n",
        "        # Train Discriminator\n",
        "        # --------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = criterion(D(real_imgs), real_labels)\n",
        "\n",
        "        z = torch.randn(batch, noise_dim).to(device)\n",
        "        fake_imgs = G(z)\n",
        "        fake_loss = criterion(D(fake_imgs.detach()), fake_labels)\n",
        "\n",
        "        D_loss = real_loss + fake_loss\n",
        "        D_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Accuracy\n",
        "        preds_real = (D(real_imgs) > 0.5).float()\n",
        "        preds_fake = (D(fake_imgs.detach()) < 0.5).float()\n",
        "        correct += preds_real.sum().item() + preds_fake.sum().item()\n",
        "        total += batch * 2\n",
        "\n",
        "        # --------------------\n",
        "        # Train Generator (TWICE, FIXED)\n",
        "        # --------------------\n",
        "        for _ in range(2):\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            z = torch.randn(batch, noise_dim).to(device)   # NEW noise\n",
        "            fake_imgs = G(z)                               # NEW graph\n",
        "\n",
        "            G_loss = criterion(D(fake_imgs), real_labels)\n",
        "            G_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        D_loss_total += D_loss.item()\n",
        "        G_loss_total += G_loss.item()\n",
        "\n",
        "    D_acc = (correct / total) * 100\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} | \"\n",
        "          f\"D_loss: {D_loss_total/len(dataloader):.3f} | \"\n",
        "          f\"D_acc: {D_acc:.2f}% | \"\n",
        "          f\"G_loss: {G_loss_total/len(dataloader):.3f}\")\n",
        "\n",
        "    # Save generated samples\n",
        "    if epoch % save_interval == 0:\n",
        "        utils.save_image(fake_imgs[:25],\n",
        "                         f\"generated_samples/epoch_{epoch:02d}.png\",\n",
        "                         nrow=5,\n",
        "                         normalize=True)\n",
        "\n",
        "# ======================================================\n",
        "# 9. GENERATE FINAL 100 IMAGES\n",
        "# ======================================================\n",
        "z = torch.randn(100, noise_dim).to(device)\n",
        "final_images = G(z)\n",
        "\n",
        "for i in range(100):\n",
        "    utils.save_image(final_images[i],\n",
        "                     f\"final_generated_images/img_{i}.png\",\n",
        "                     normalize=True)\n",
        "\n",
        "# ======================================================\n",
        "# 10. SIMPLE CLASSIFIER\n",
        "# ======================================================\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "classifier = Classifier().to(device)\n",
        "optimizer_C = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(3):\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer_C.zero_grad()\n",
        "        loss = loss_fn(classifier(imgs), labels)\n",
        "        loss.backward()\n",
        "        optimizer_C.step()\n",
        "\n",
        "# ======================================================\n",
        "# 11. LABEL PREDICTION\n",
        "# ======================================================\n",
        "with torch.no_grad():\n",
        "    preds = classifier(final_images).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "label_counts = Counter(preds)\n",
        "\n",
        "print(\"\\nLabel Distribution of Generated Images:\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"Label {label}: {count}\")"
      ]
    }
  ]
}